{"version":3,"file":".pnpm/micromark-util-subtokenize@1.0.2/node_modules/micromark-util-subtokenize.c39f805fae6b207d059f.js","mappings":"4JAaO,SAASA,EAAYC,GAE1B,MAAMC,EAAQ,GACd,IAGIC,EAGAC,EAGAC,EAGAC,EAGAC,EAGAC,EAGAC,EArBAC,GAAS,EAuBb,OAASA,EAAQT,EAAOU,QAAQ,CAC9B,KAAOD,KAASR,GACdQ,EAAQR,EAAMQ,GAMhB,GAHAP,EAAQF,EAAOS,GAIbA,GACkB,cAAlBP,EAAM,GAAGS,MACqB,mBAA9BX,EAAOS,EAAQ,GAAG,GAAGE,OAErBJ,EAAYL,EAAM,GAAGU,WAAWZ,OAChCI,EAAa,EAGXA,EAAaG,EAAUG,QACW,oBAAlCH,EAAUH,GAAY,GAAGO,OAEzBP,GAAc,GAIdA,EAAaG,EAAUG,QACW,YAAlCH,EAAUH,GAAY,GAAGO,MAEzB,OAASP,EAAaG,EAAUG,QACQ,YAAlCH,EAAUH,GAAY,GAAGO,MAIS,cAAlCJ,EAAUH,GAAY,GAAGO,OAC3BJ,EAAUH,GAAY,GAAGS,6BAA8B,EACvDT,KAMR,GAAiB,UAAbF,EAAM,GACJA,EAAM,GAAGY,cACXC,OAAOC,OAAOf,EAAOgB,EAAWjB,EAAQS,IACxCA,EAAQR,EAAMQ,GACdD,GAAO,QAGN,GAAIN,EAAM,GAAGgB,WAAY,CAI5B,IAHAd,EAAaK,EACbN,OAAYgB,EAELf,MACLC,EAAaL,EAAOI,GAGK,eAAvBC,EAAW,GAAGM,MACS,oBAAvBN,EAAW,GAAGM,OAEQ,UAAlBN,EAAW,KACTF,IACFH,EAAOG,GAAW,GAAGQ,KAAO,mBAG9BN,EAAW,GAAGM,KAAO,aACrBR,EAAYC,GAOdD,IAEFD,EAAM,GAAGkB,IAAML,OAAOC,OAAO,GAAIhB,EAAOG,GAAW,GAAGkB,OAEtDf,EAAaN,EAAOsB,MAAMnB,EAAWM,GACrCH,EAAWiB,QAAQrB,IACnB,OAAOF,EAAQG,EAAWM,EAAQN,EAAY,EAAGG,KAKvD,OAAQE,EAUV,SAASS,EAAWjB,EAAQwB,GAC1B,MAAMC,EAAQzB,EAAOwB,GAAY,GAC3BE,EAAU1B,EAAOwB,GAAY,GACnC,IAAIG,EAAgBH,EAAa,EAGjC,MAAMI,EAAiB,GACjBC,EACJJ,EAAMb,YAAcc,EAAQI,OAAOL,EAAMX,aAAaW,EAAMJ,OACxDU,EAAcF,EAAU7B,OAGxBC,EAAQ,GAGR+B,EAAO,GAGb,IAAIC,EAGAC,EACAzB,GAAS,EAGT0B,EAAUV,EACVW,EAAS,EACTf,EAAQ,EACZ,MAAMgB,EAAS,CAAChB,GAGhB,KAAOc,GAAS,CAEd,KAAOnC,IAAS2B,GAAe,KAAOQ,IAItCP,EAAeU,KAAKX,GAEfQ,EAAQvB,aACXqB,EAASP,EAAQa,YAAYJ,GAExBA,EAAQK,MACXP,EAAOK,KAAK,MAGVJ,GACFL,EAAUY,WAAWN,EAAQd,OAG3Bc,EAAQtB,8BACVgB,EAAUa,oCAAqC,GAGjDb,EAAUc,MAAMV,GAEZE,EAAQtB,8BACVgB,EAAUa,wCAAqCvB,IAInDe,EAAWC,EACXA,EAAUA,EAAQK,KAMpB,IAFAL,EAAUV,IAEDhB,EAAQsB,EAAYrB,QAGC,SAA1BqB,EAAYtB,GAAO,IACW,UAA9BsB,EAAYtB,EAAQ,GAAG,IACvBsB,EAAYtB,GAAO,GAAGE,OAASoB,EAAYtB,EAAQ,GAAG,GAAGE,MACzDoB,EAAYtB,GAAO,GAAGY,MAAMuB,OAASb,EAAYtB,GAAO,GAAGW,IAAIwB,OAE/DvB,EAAQZ,EAAQ,EAChB4B,EAAOC,KAAKjB,GAEZc,EAAQvB,gBAAaO,EACrBgB,EAAQD,cAAWf,EACnBgB,EAAUA,EAAQK,MAmBtB,IAfAX,EAAU7B,OAAS,GAIfmC,GAEFA,EAAQvB,gBAAaO,EACrBgB,EAAQD,cAAWf,GAEnBkB,EAAOQ,MAITpC,EAAQ4B,EAAO3B,OAERD,KAAS,CACd,MAAMa,EAAQS,EAAYT,MAAMe,EAAO5B,GAAQ4B,EAAO5B,EAAQ,IACxDY,EAAQO,EAAeiB,MAC7B5C,EAAMsB,QAAQ,CAACF,EAAOA,EAAQC,EAAMZ,OAAS,KAC7C,OAAOV,EAAQqB,EAAO,EAAGC,GAK3B,IAFAb,GAAS,IAEAA,EAAQR,EAAMS,QACrBsB,EAAKI,EAASnC,EAAMQ,GAAO,IAAM2B,EAASnC,EAAMQ,GAAO,GACvD2B,GAAUnC,EAAMQ,GAAO,GAAKR,EAAMQ,GAAO,GAAK,EAGhD,OAAOuB","sources":["webpack://tech-stack/./node_modules/.pnpm/micromark-util-subtokenize@1.0.2/node_modules/micromark-util-subtokenize/index.js"],"sourcesContent":["/**\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').Chunk} Chunk\n * @typedef {import('micromark-util-types').Event} Event\n */\nimport {splice} from 'micromark-util-chunked'\n\n/**\n * Tokenize subcontent.\n *\n * @param {Event[]} events\n * @returns {boolean}\n */\nexport function subtokenize(events) {\n  /** @type {Record<string, number>} */\n  const jumps = {}\n  let index = -1\n  /** @type {Event} */\n\n  let event\n  /** @type {number|undefined} */\n\n  let lineIndex\n  /** @type {number} */\n\n  let otherIndex\n  /** @type {Event} */\n\n  let otherEvent\n  /** @type {Event[]} */\n\n  let parameters\n  /** @type {Event[]} */\n\n  let subevents\n  /** @type {boolean|undefined} */\n\n  let more\n\n  while (++index < events.length) {\n    while (index in jumps) {\n      index = jumps[index]\n    }\n\n    event = events[index] // Add a hook for the GFM tasklist extension, which needs to know if text\n    // is in the first content of a list item.\n\n    if (\n      index &&\n      event[1].type === 'chunkFlow' &&\n      events[index - 1][1].type === 'listItemPrefix'\n    ) {\n      subevents = event[1]._tokenizer.events\n      otherIndex = 0\n\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'lineEndingBlank'\n      ) {\n        otherIndex += 2\n      }\n\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'content'\n      ) {\n        while (++otherIndex < subevents.length) {\n          if (subevents[otherIndex][1].type === 'content') {\n            break\n          }\n\n          if (subevents[otherIndex][1].type === 'chunkText') {\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true\n            otherIndex++\n          }\n        }\n      }\n    } // Enter.\n\n    if (event[0] === 'enter') {\n      if (event[1].contentType) {\n        Object.assign(jumps, subcontent(events, index))\n        index = jumps[index]\n        more = true\n      }\n    } // Exit.\n    else if (event[1]._container) {\n      otherIndex = index\n      lineIndex = undefined\n\n      while (otherIndex--) {\n        otherEvent = events[otherIndex]\n\n        if (\n          otherEvent[1].type === 'lineEnding' ||\n          otherEvent[1].type === 'lineEndingBlank'\n        ) {\n          if (otherEvent[0] === 'enter') {\n            if (lineIndex) {\n              events[lineIndex][1].type = 'lineEndingBlank'\n            }\n\n            otherEvent[1].type = 'lineEnding'\n            lineIndex = otherIndex\n          }\n        } else {\n          break\n        }\n      }\n\n      if (lineIndex) {\n        // Fix position.\n        event[1].end = Object.assign({}, events[lineIndex][1].start) // Switch container exit w/ line endings.\n\n        parameters = events.slice(lineIndex, index)\n        parameters.unshift(event)\n        splice(events, lineIndex, index - lineIndex + 1, parameters)\n      }\n    }\n  }\n\n  return !more\n}\n/**\n * Tokenize embedded tokens.\n *\n * @param {Event[]} events\n * @param {number} eventIndex\n * @returns {Record<string, number>}\n */\n\nfunction subcontent(events, eventIndex) {\n  const token = events[eventIndex][1]\n  const context = events[eventIndex][2]\n  let startPosition = eventIndex - 1\n  /** @type {number[]} */\n\n  const startPositions = []\n  const tokenizer =\n    token._tokenizer || context.parser[token.contentType](token.start)\n  const childEvents = tokenizer.events\n  /** @type {[number, number][]} */\n\n  const jumps = []\n  /** @type {Record<string, number>} */\n\n  const gaps = {}\n  /** @type {Chunk[]} */\n\n  let stream\n  /** @type {Token|undefined} */\n\n  let previous\n  let index = -1\n  /** @type {Token|undefined} */\n\n  let current = token\n  let adjust = 0\n  let start = 0\n  const breaks = [start] // Loop forward through the linked tokens to pass them in order to the\n  // subtokenizer.\n\n  while (current) {\n    // Find the position of the event for this token.\n    while (events[++startPosition][1] !== current) {\n      // Empty.\n    }\n\n    startPositions.push(startPosition)\n\n    if (!current._tokenizer) {\n      stream = context.sliceStream(current)\n\n      if (!current.next) {\n        stream.push(null)\n      }\n\n      if (previous) {\n        tokenizer.defineSkip(current.start)\n      }\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = true\n      }\n\n      tokenizer.write(stream)\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined\n      }\n    } // Unravel the next token.\n\n    previous = current\n    current = current.next\n  } // Now, loop back through all events (and linked tokens), to figure out which\n  // parts belong where.\n\n  current = token\n\n  while (++index < childEvents.length) {\n    if (\n      // Find a void token that includes a break.\n      childEvents[index][0] === 'exit' &&\n      childEvents[index - 1][0] === 'enter' &&\n      childEvents[index][1].type === childEvents[index - 1][1].type &&\n      childEvents[index][1].start.line !== childEvents[index][1].end.line\n    ) {\n      start = index + 1\n      breaks.push(start) // Help GC.\n\n      current._tokenizer = undefined\n      current.previous = undefined\n      current = current.next\n    }\n  } // Help GC.\n\n  tokenizer.events = [] // If there’s one more token (which is the cases for lines that end in an\n  // EOF), that’s perfect: the last point we found starts it.\n  // If there isn’t then make sure any remaining content is added to it.\n\n  if (current) {\n    // Help GC.\n    current._tokenizer = undefined\n    current.previous = undefined\n  } else {\n    breaks.pop()\n  } // Now splice the events from the subtokenizer into the current events,\n  // moving back to front so that splice indices aren’t affected.\n\n  index = breaks.length\n\n  while (index--) {\n    const slice = childEvents.slice(breaks[index], breaks[index + 1])\n    const start = startPositions.pop()\n    jumps.unshift([start, start + slice.length - 1])\n    splice(events, start, 2, slice)\n  }\n\n  index = -1\n\n  while (++index < jumps.length) {\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1]\n    adjust += jumps[index][1] - jumps[index][0] - 1\n  }\n\n  return gaps\n}\n"],"names":["subtokenize","events","jumps","event","lineIndex","otherIndex","otherEvent","parameters","subevents","more","index","length","type","_tokenizer","_isInFirstContentOfListItem","contentType","Object","assign","subcontent","_container","undefined","end","start","slice","unshift","eventIndex","token","context","startPosition","startPositions","tokenizer","parser","childEvents","gaps","stream","previous","current","adjust","breaks","push","sliceStream","next","defineSkip","_gfmTasklistFirstContentOfListItem","write","line","pop"],"sourceRoot":""}